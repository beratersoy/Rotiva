"""
RAG Engine - Retrieval-Augmented Generation Motoru (LangChain Powered)
KullanÄ±cÄ± sorgusuna gÃ¶re alakalÄ± etkinlikleri bulup, Gemini AI ile yanÄ±t Ã¼retir

PROJE GEREKSÄ°NÄ°MLERÄ°:
âœ… Generation Model: Google Gemini 2.5 Flash (LangChain integration)
âœ… Embedding Model: Sentence-Transformers (paraphrase-multilingual)
âœ… Vektor Database: FAISS
âœ… RAG Framework: LangChain
"""

from rag_pipeline.retriever import FAISSRetriever
from utils.llm_client import OpenAIClient
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_google_genai import ChatGoogleGenerativeAI
import os


class RAGEngine:
    """
    LangChain tabanlÄ± RAG (Retrieval-Augmented Generation) pipeline
    FAISS vektor DB + Embeddings + Gemini AI = AkÄ±llÄ± etkinlik asistanÄ±
    """
    
    def __init__(self, events):
        # ğŸ—„ï¸ FAISS + Embedding tabanlÄ± arama motoru (yeni!)
        self.retriever = FAISSRetriever(events)
        
        # ğŸ¤– Gemini AI istemcisi (eski sistem iÃ§in yedek)
        self.llm_client = OpenAIClient()
        
        # ğŸ¦œ LangChain + Gemini entegrasyonu (proje gereksinimi)
        api_key = os.getenv('GEMINI_API_KEY', 'AIzaSyBKe8kUhQIV6kOYZxtW6Bfc9KVoUVIdAsc')
        self.langchain_llm = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=api_key,
            temperature=0.7
        )
        
        # ğŸ“ LangChain Prompt Template (LÄ°NK DESTEKLÄ°)
        self.prompt_template = PromptTemplate(
            input_variables=["context", "question"],
            template="""Sen Rotiva AI'sÄ±n, TÃ¼rkiye'deki etkinliklerin uzmanÄ± bir asistansÄ±n.

KullanÄ±cÄ±nÄ±n sorusuna aÅŸaÄŸÄ±daki etkinlik bilgilerine dayanarak yanÄ±t ver:

{context}

KullanÄ±cÄ± Sorusu: {question}

YanÄ±t KurallarÄ±:
- DoÄŸal, samimi ve yardÄ±msever ol
- Emoji kullan (ğŸ­ğŸµğŸ¬ğŸ¨)
- Her etkinlik yeni satÄ±rda
- Ã–NEMLÄ°: Etkinlik adÄ±nÄ± [Etkinlik Ä°smi](link) formatÄ±nda markdown link yap
- Link varsa mutlaka kullan, yoksa sadece etkinlik ismi yaz
- Tarih, yer ve kaynak bilgilerini paylaÅŸ
- KullanÄ±cÄ±ya soru sor (ilgi alanlarÄ±nÄ± keÅŸfet)

YanÄ±t:"""
        )
        
        # ğŸ”— LangChain ile RAG zinciri oluÅŸtur
        self.rag_chain = LLMChain(
            llm=self.langchain_llm,
            prompt=self.prompt_template
        )
        
        print("âœ… LangChain RAG Engine baÅŸlatÄ±ldÄ± (FAISS + Embeddings + Gemini)\n")
    
    def answer_question(self, query, city_filter=None, top_k=5):
        """
        KullanÄ±cÄ± sorusuna LangChain RAG tabanlÄ± yanÄ±t Ã¼ret
        
        Args:
            query: KullanÄ±cÄ±nÄ±n sorusu
            city_filter: Åehir filtresi (Ã¶rn: "Kocaeli", "Sakarya")
            top_k: KaÃ§ etkinlik alÄ±nacak (varsayÄ±lan: 5)
        
        Returns:
            dict: {'answer': str, 'sources': list} - AI yanÄ±tÄ± ve kullanÄ±lan kaynaklar
        """
        try:
            # 1. RETRIEVAL: En alakalÄ± etkinlikleri bul (FAISS + Embeddings ile semantik arama)
            results = self.retriever.retrieve(query, k=top_k, city_filter=city_filter)
            
            if not results:
                return {
                    'answer': 'ÃœzgÃ¼nÃ¼m, kriterlerine uygun etkinlik bulunamadÄ±. FarklÄ± bir arama yapmak ister misin?',
                    'sources': []
                }
            
            # 2. CONTEXT OLUÅTURMA: Bulunan etkinlikleri metin formatÄ±na Ã§evir (LÄ°NK DAHÄ°L)
            context_text = "Ä°lgili Etkinlikler:\n\n"
            for i, result in enumerate(results, 1):
                event = result['event']
                context_text += f"{i}. {event['title']}\n"
                context_text += f"   Tarih: {event['date']}\n"
                context_text += f"   Yer: {event['location']}\n"
                context_text += f"   Åehir: {event['city']}\n"
                context_text += f"   Link: {event.get('link', 'Yok')}\n"
                context_text += f"   Kaynak: {event['source']}\n\n"
            
            # 3. GENERATION: LangChain + Gemini AI ile doÄŸal dil yanÄ±tÄ± Ã¼ret (PROJE GEREKSÄ°NÄ°MÄ°)
            try:
                # LangChain RAG chain kullan
                response = self.rag_chain.invoke({
                    "context": context_text,
                    "question": query
                })
                answer = response['text'] if isinstance(response, dict) else str(response)
            except Exception as lc_error:
                # LangChain hatasÄ± olursa, eski sisteme geri dÃ¶n
                print(f"âš ï¸ LangChain hatasÄ±, fallback aktif: {lc_error}")
                answer = self.llm_client.generate_response(context_text, query)
            
            return {
                'answer': answer,
                'sources': [r['event'] for r in results]
            }
            
        except Exception as e:
            return {
                'answer': f"Hata: {e}",
                'sources': []
            }